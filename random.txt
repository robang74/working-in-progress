### rationale #################################

Collecting entropy is a metaphor because only information can be "collected"
ergo stored in a precise notation like the number 123 or a string of bits. The
aim of "collecting entropy" is to have a sequence of numbers (information) which
cannot be anticipated. However, as you can imagine because that series of
numbers is stored somewhere, it is possible to anticipate because it is known.

The fundamental point is about WHO can access that information and HOW that
information is structured. The structure of information is fundamental to avoid
(or minimise) the risk that someone can guess the sequence. For example, we can
use the Fibonacci series and decide to start from a specific number of the
original series instead of 1. Unfortunately everyone that can see or guess a
single number of the series knows all the rest of the series. We can use the
Fibonacci series from N and step 2, or step N-4, or whatever, it has a
structure.

Structured information can be guessed, it is just a matter of how difficult it
is to make that guess (or given a unit of computation power per second, how many
seconds it will take). This is related to how complicated is the relationship
between a number and the number after e.g. Fibonacci(N, 4). When this
relationship is "random" and in particular "white noise random" it means that
every next 8-bit (or 16-bit, or 32-bit) of information are totally uncorrelated
with anything before and after thus the chance to guess the next 8-bit string is
1:2^8 (it cannot be more). This is randomness, white noise.

The 'drama' is about HOW to generate a random sequence of numbers from a system
that has been designed to be predictable and deterministic (and also error auto
correcting like ECC memory, for example). The "easiest" answer is having a very
sensitive thermometer (e.g. 0.00001Â°C precision) and collecting each dT-time the
least significant digit of temperature measuring. Each dT is a fixed and
repetitive timing and precision is a supposition plus the wires are EM emitters.
Which in practice is easier to sniff than we might suppose because extracting
"entropy" from heat requires physical work (L) or a Maxwell's devil.

Under this perspective what really matters is the predictability from the known
(so how hard is to guess from the known or plausible known) and how difficult is
to access that information (storing safety). Is it stored inside the CPU
registry? The chip is the safe boundary. Into the memory? The EM S/N of
conductive tracks between CPU and RAM is the safe boundary. In the kernel space
or in the user-land? For a local attacker, it is the root privilege in local
escalation, the safe boundary. For a remote, wait... Here is the MAIN point.

This "obscurity by randomness" is not used to protect the root password secret
only but mainly to protect the ciphering of the communications. Because a crypto
algorithm can be extremely hard to break unless guessing the seed of the random
numbers generator is easy instead. Because the idea behind the cripto-safety is
about who knows the key (a string of bits) can de/cypher the message (Enigma,
WW2) despite the algorithm for de/cypher being public (or desveiled, because
leaked).

Therefore about randomness, everything is about HOW hard it is to predict or
sniff the sequence of numbers from what we already know or can reasonably
supporsing and check in a very short time, short enough to be useful: 1s or
1E(1E100)s?

In modern systems the CPU is a multi-core, multi-thread, multi-branch and all of
these "features" are multi-layered cached. Many tasks are running even if the
whole userland system is "doing nothing" and task preemption can kick in at any
time, creating latencies in HOW long a deterministic task takes. Because also in
complex systems it often requires a very-low latency (e.g. industry and CNC),
also these latencies aren't very unpredictable, and usually they aren't but
low.

Fortunately, the latency isn't ALWAYS the same nanoseconds precise and the
variance of a latency is its jitter, e.g. a latency in [ 80, 120 ] ns has a 20
(or 40) ns of maximum jitter. Usually, the distribution is a Bell curve so the
dt = 1ns (actual jitter) is more frequent than dt = 10ns. The main idea could be
that within [ 95, 105 ] ns the jitter distribution is almost flat so every value
in that range is almost the same probable to pop-up. When the value is read
outside that interval then it is discarded thus not stored. At the next turn it
might be discarded again and the chance of a continuous rejection which stops
the sequence has a not-zero probability to happen. But dT % 10 is always flat
when the average is 100 and the latency is 40 (T >> dT >> %dt).

A small number rests on a non-white noise number but Cauchy^2 distributed is
theoretically totally flat in terms of distribution, and also in practice when
the T >> dT >> %dt is related by a 4x or 10x rations like the LSB of 32-bit.

Two functions are great to multiply "entropy": we cannot create entropy by a
determinist system but we can multiply it because we can spread that little bit
of randomness over a larger set of bits with a hash and zip algorithm.

Because the hash "diffuses" in a fast but NP-hard to recover way, and the zip
algorithm provides a nearly flat distribution of values which are "hard" to
guess when the first part of the output (the table) is lost (until the next
chunk). For example if 128Kb is the input chunk and 4x is the max compression
rate expected in a single chunk the output chunk will be 32Kb thus a 64b pick
has a great chance to fall somewhere the unpredictability is almost random
flat.

The zip algorithm provides a (1) nearly flat distribution of values which is (2)
hard to guess without knowing the encoding table by definition of "zip" itself.
A decent zip algorithm should tear down the redundancy of the input granting the
output values distribution would be nearly flat (file header apart, etc) as (1).
A decent zip algorithm whose output can be predicted once lost the encoding
table means that the zipped part is not p-flat enough and zip is very poor in
size shrinking or fidelity (e.g., it is a copy not a zipping or zeros string).

A process PT that creates a relatively large jittered latency and clogging the
CPU multi-threading management for a little while its execution thus being
scattered among internal computational pipes and caches, interrupted and
pre-empted is a great "random" generator in terms of nanoseconds fluctuation.

Thus the PT start, stop time in nanoseconds are not related in the least
significant stop digits. Express these two numbers in digits 0-9 then the string
$stop$start has the LSB randomness in the middle of the string which will be
something like: nnXnnx. Because the start can be guessed apart a few nanoseconds
but the relatively large jitter causes the LSB stop part to have many more
digits that can be considered strongly random in their nature.

Once a string which the random nature of nnXnnx is hashed the result is XXXXXX
becuase nobody can anymore say (once the start/stop are lost) how the randomness
has been diffused into the checksum string length, it is everywhere making each
bit totally unpredictable in the same manner (multiplication by diffusion).

If a process like the one described here is operated by the CPU microcode, it
would be safely bound by the CPU itself, a modern CPUs can be seen as very good
random generators because of their jitter unpredictability in executing tasks.

How good is this kind of random generator based on CPU arch and  microcode to be
good at providing unbreakable cryptography assuming the whole process is made at
the state of art? Who design the chip (or made the chip) and deliver the
microcode can definitely have access to every ciphered communications no matter
what.

### original function (2023) ##################

{
  n=$((33 + ${RANDOM:-15}%32))
  dd if=/dev/random bs=$n count=1 2>&1
  cat /proc/cmdline /proc/*stat /init*
} | pigz -$((1 + n%9))c > /dev/urandom &

### untested functions (2026) #################

rafrand_zro() {
  echo $((33 + ${1:-$RANDOM}%32))
}

rafrand_one() {
  k=$(rafrand_zro ${1:-}); echo ${1:-}
  dd if=/dev/random bs=$k count=1 2>&1
  cat /proc/cmdline /proc/*stat /init*
}

rafrand_two() {
  n=$(rafrand_zro ${1:-})
  m=$((9 - ${2:-$RANDOM}%9))
  rafrand_one ${3:-} |\
    pigz -${m}cp$((4 * $(nproc))) -b128 |\
      dd ibs=$n skip=1 obs=64 count=1 status=none
}

rafrand() {
  a=$(date +%N)
  c=$(rafrand_two | md5sum)
  b=$(date +%N)
  c=$(echo $b$c$a | md5sum | tr [0a-f-\ ] [1-9])
  a=$(echo $c | cut -c1-5)
  b=$(echo $c | cut -c6-10)
  c=$(echo $c | cut -c11-15)
  rafrand_two $c $a $b >/dev/urandom
}

