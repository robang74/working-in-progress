(c) 2026, Roberto A. Foglietta <roberto.foglietta@gmail.com>, CC ND-BY-NC 4.0

### rationale #################################

Collecting entropy is a metaphor because only information can be "collected"
ergo stored in a precise notation like the number 123 or a string of bits. The
aim of "collecting entropy" is to have a sequence of numbers (information) which
cannot be anticipated. However, as you can imagine because that series of
numbers is stored somewhere, it is possible to anticipate because it is known.

The fundamental point is about WHO can access that information and HOW that
information is structured. The structure of information is fundamental to avoid
(or minimise) the risk that someone can guess the sequence. For example, we can
use the Fibonacci series and decide to start from a specific number of the
original series instead of 1. Unfortunately everyone that can see or guess a
single number of the series knows all the rest of the series. We can use the
Fibonacci series from N and step 2, or step N-4, or whatever, it has a
structure.

Structured information can be guessed, it is just a matter of how difficult it
is to make that guess (or given a unit of computation power per second, how many
seconds it will take). This is related to how complicated is the relationship
between a number and the number after e.g. Fibonacci(N, 4). When this
relationship is "random" and in particular "white noise random" it means that
every next 8-bit (or 16-bit, or 32-bit) of information are totally uncorrelated
with anything before and after thus the chance to guess the next 8-bit string is
1:2^8 (it cannot be more). This is randomness, white noise.

The 'drama' is about HOW to generate a random sequence of numbers from a system
that has been designed to be predictable and deterministic (and also error auto
correcting like ECC memory, for example). The "easiest" answer is having a very
sensitive thermometer (e.g. 0.00001Â°C precision) and collecting each dT-time the
least significant digit of temperature measuring. Each dT is a fixed and
repetitive timing and precision is a supposition plus the wires are EM emitters.
Which in practice is easier to sniff than we might suppose because extracting
"entropy" from heat requires physical work (L) or a Maxwell's devil.

Under this perspective what really matters is the predictability from the known
(so how hard is to guess from the known or plausible known) and how difficult is
to access that information (storing safety). Is it stored inside the CPU
registry? The chip is the safe boundary. Into the memory? The EM S/N of
conductive tracks between CPU and RAM is the safe boundary. In the kernel space
or in the user-land? For a local attacker, it is the root privilege in local
escalation, the safe boundary. For a remote, wait... Here is the MAIN point.

This "obscurity by randomness" is not used to protect the root password secret
only but mainly to protect the ciphering of the communications. Because a crypto
algorithm can be extremely hard to break unless guessing the seed of the random
numbers generator is easy instead. Because the idea behind the cripto-safety is
about who knows the key (a string of bits) can de/cypher the message (Enigma,
WW2) despite the algorithm for de/cypher being public (or desveiled, because
leaked).

Therefore about randomness, everything is about HOW hard it is to predict or
sniff the sequence of numbers from what we already know or can reasonably
supporsing and check in a very short time, short enough to be useful: 1s or
1E(1E100)s?

In modern systems the CPU is a multi-core, multi-thread, multi-branch and all of
these "features" are multi-layered cached. Many tasks are running even if the
whole userland system is "doing nothing" and task preemption can kick in at any
time, creating latencies in HOW long a deterministic task takes. Because also in
complex systems it often requires a very-low latency (e.g. industry and CNC),
also these latencies aren't very unpredictable, and usually they aren't but
low.

Fortunately, the latency isn't ALWAYS the same nanoseconds precise and the
variance of a latency is its jitter, e.g. a latency in [ 80, 120 ] ns has a 20
(or 40) ns of maximum jitter. Usually, the distribution is a Bell curve so the
dt = 1ns (actual jitter) is more frequent than dt = 10ns. The main idea could be
that within [ 95, 105 ] ns the jitter distribution is almost flat so every value
in that range is almost the same probable to pop-up. When the value is read
outside that interval then it is discarded thus not stored. At the next turn it
might be discarded again and the chance of a continuous rejection which stops
the sequence has a not-zero probability to happen. But dT % 10 is always flat
when the average is 100 and the latency is 40 (T >> dT >> %dt).

A small number rests on a non-white noise number but Cauchy^2 distributed is
theoretically totally flat in terms of distribution, and also in practice when
the T >> dT >> %dt is related by a 4x or 10x rations like the LSB of 32-bit.

  https://github.com/robang74/roberto-a-foglietta/
    /blob/main/data/Tesi_Foglietta-051a.pdf (jitter)

Two functions are great to multiply "entropy": we cannot create entropy by a
determinist system but we can multiply it because we can spread that little bit
of randomness over a larger set of bits with a hash and zip algorithm.

Because the hash "diffuses" in a fast but NP-hard to recover way, and the zip
algorithm provides a nearly flat distribution of values which are "hard" to
guess when the first part of the output (the table) is lost (until the next
chunk). For example if 128Kb is the input chunk and 4x is the max compression
rate expected in a single chunk the output chunk will be 32Kb thus a 64b pick
has a great chance to fall somewhere the unpredictability is almost random
flat.

The zip algorithm provides a (1) nearly flat distribution of values which is (2)
hard to guess without knowing the encoding table by definition of "zip" itself.
A decent zip algorithm should tear down the redundancy of the input granting the
output values distribution would be nearly flat (file header apart, etc) as (1).
A decent zip algorithm whose output can be predicted once lost the encoding
table means that the zipped part is not p-flat enough and zip is very poor in
size shrinking or fidelity (e.g., it is a copy not a zipping or zeros string).

A process PT that creates a relatively large jittered latency and clogging the
CPU multi-threading management for a little while its execution thus being
scattered among internal computational pipes and caches, interrupted and
pre-empted is a great "random" generator in terms of nanoseconds fluctuation.

Thus the PT start, stop time in nanoseconds are not related in the least
significant stop digits. Express these two numbers in digits 0-9 then the string
$stop$start has the LSB randomness in the middle of the string which will be
something like: nnXnnx. Because the start can be guessed apart a few nanoseconds
but the relatively large jitter causes the LSB stop part to have many more
digits that can be considered strongly random in their nature.

Once a string which the random nature of nnXnnx is hashed the result is XXXXXX
becuase nobody can anymore say (once the start/stop are lost) how the randomness
has been diffused into the checksum string length, it is everywhere making each
bit totally unpredictable in the same manner (multiplication by diffusion).

If a process like the one described here is operated by the CPU microcode, it
would be safely bound by the CPU itself, a modern CPUs can be seen as very good
random generators because of their jitter unpredictability in executing tasks.

How good is this kind of random generator based on CPU arch and  microcode to be
good at providing unbreakable cryptography assuming the whole process is made at
the state of art? Who design the chip (or made the chip) and deliver the
microcode can definitely have access to every ciphered communications no matter
what.

### original function (2023) ##################

{
  n=$((33 + ${RANDOM:-15}%32))
  dd if=/dev/random bs=$n count=1 2>&1
  cat /proc/cmdline /proc/*stat /init*
} | pigz -$((1 + n%9))c > /dev/urandom &

### untested functions (2026) #################

rafrand_zro() {
  echo $((33 + ${1:-$RANDOM}%32))
}

rafrand_ubf() {
  cmd=""; if [ -n "$1" ]; then cmd="chrt -$1"; fi; shift
  ionice -c3 $cmd stdbuf -o0 -e0 "$@"
}

rafrand_one() {
  k=$(rafrand_zro ${1:-})
  echo ${1:-} & ( set -x
    rafrand_ubf "" dd if=/dev/random bs=$k count=1 &
    rafrand_ubf "i 0" tail -n$((k*16)) /var/log/syslog & {
      for f in $(ls -1 /proc/cmdline /proc/*stat /init*)
        do rafrand_ubf "b 0" cat $f & done &
    }
  ) 2>&1
}

rafrand_two() {
  n=$(rafrand_zro ${1:-})
  m=$((9 - ${2:-$RANDOM}%9))
  rafrand_one ${3:-} |{\
    pigz -${m}cp$((4 * $(nproc))) -b32 |\
      dd bs=1 skip=$n count=64
    } 2>/dev/null
}

rafrand() {
  a=$(date +%N)
  c=$(rafrand_two | md5sum)
  b=$(date +%N)
  c=$(echo $b$a$c | md5sum | tr [0a-f-\ ] [1-9])
  a=$(echo $c | cut -c1-5)
  b=$(echo $c | cut -c6-10)
  c=$(echo $c | cut -c11-15)
  rafrand_two $c $a $b >/dev/urandom
}

### apparent issues ###########################

1. The "tr" Command Entropy Bias

Theoretically correct to arise a "bias issue" but that string is splitted
in 3 parts and that parts are used as seed for other strong-level randomness
production tasks. While the randomness seed seems very essential, the values
used in that process are relatively small compared to the 5 digits (%32 to %3)
hence it doesn't matter because reminders are those that really matter.

2. The "dd" skipping on Compressed Stream

The variable "n" controls the read size of the compressed data and it creates
a dependency where if the compressed output is smaller than $n (unlikely here
but possible in other contexts), the command fails/outputs nothing.

Which is correct, in theory. However, the main issue relies in "cat" files that
can be void or not existent which is the reason because 2>&1 has been used to
"at least" collect something and the "echo ${1:-}" has been added for a little
more data because cat might partially or totally fail while /dev/random is not
supposed to provide a lot of data but just a bit of extra-initial randomness.

However, the shell implementation is just a PoC because the C-language is
the way. Even in the laziest option, the shell implementation should be
contextualised on the target system which, for example, it might not have
"pigz" or not having a multi-thread processor but a "dumb" 32-bit micro
which provides no significant jitters or "pigz" would be almost useless
for goal in that system.

### updates ###################################

rafrand_one:

  It spawns many processes, it uses "2>&1", "set -x", to mix by a time task
  switch unpredictable sequence different data flow which can potentially be
  interlivead "stdbuf" into a single output stream not only because task
  switching but also because output buffering suppresion (or changes).

rafrand_ubf:

  Another way to introduce task switching uncertainty settings scheduling
  policy and priorities. Both should be tuned or rely on templates specific
  for the target architecture and customisable for the specific system. This
  approach has a good chance to "collect entropy" also from simpler CPUs.

  By the way, leveraging a "complex" scheduler is an assumption rather than
  just having a simply rotating tasks scheduler into the OS. However, rotating
  tasks may introduce some glitches which bring in those "stochastics" features
  we would like to have.

  It is worth to note that because of just a small portion of the zipped data
  stream is taken in account, it is hard to believe that this would have a
  practical effect unless hashing chunks would not also included into that
  data flows mixing process.

### tiny random generator #####################

The rafrand_tiny() leverages rafgen5sum() which produce random output by
multiplying I/O task jittering like dd output like this:

  echo | dd
  0+1 records in
  0+1 records out
  1 byte copied, 4.5698e-05 s, 21.9 kB/s

there are few randonmess here because execution time varies between 4 and 8 -05s
on my tests which are 4.5 digits 5E4 combinations, less than 16 bits of "entropy"
because everyhting else is correlated at that 4.5 digits of informations.

  rafgen5sum() {
      n=${1:-5}; echo | while true; do
          let n-- || break;
          dd bs=1 count=1k 2>&1 | md5sum
      done
  }

Since rafgen5sum() runs that dd five times, it creates a data stream with 64+ bits
of unstructured information (randomness) which is enough to fill the 128 md5sum.

  ug() { rafgen5sum | md5sum; }  # 128-bit random

  ng() { { ug;ug;ug;ug; }| pigz -11cp8 | dd bs=32 skip=1 count=2 status=none; }

Here below a convoluted way of doing the same of ng() probably over-engineered.

  rafrand_tiny() {
    for i in $(seq 1 ${1:-4}); do
      rafgen5sum | pigz -1cp8 | md5sum
    done | pigz -11cp8 |\
      dd bs=32 skip=1 count=2 status=none
  }

Hint: commands "wc -c" and "od -x" are your friends.

